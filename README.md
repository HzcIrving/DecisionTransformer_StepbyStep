# DecisionTransformer_StepbyStep

## Intro

Decision Transformer: A brand new Offline RL Pattern.

è¿™æ˜¯å…³äº[NeurIPS 2021 ](https://paperswithcode.com/conference/neurips-2021-12)çƒ­é—¨è®ºæ–‡Decision Transformerçš„å¤ç°ã€‚

ğŸ‘ åŸæ–‡åœ°å€: [Decision Transformer: Reinforcement Learning via Sequence Modeling](http://proceedings.neurips.cc/paper/2021/file/7f489f642a0ddb10272b5c31057f0663-Paper.pdf)

ğŸ‘ å®˜æ–¹çš„Gitä»“åº“: [decision-transformer(official) ](https://github.com/kzl/decision-transformer)

## Decision Transformer

Decision Transformerå±äºOffline RLï¼Œæ‰€è°“Offline RLï¼Œå³ä»æ¬¡ä¼˜æ•°æ®ä¸­å­¦ä¹ ç­–ç•¥æ¥åˆ†é…Agentï¼Œå³ä»å›ºå®šã€æœ‰é™çš„ç»éªŒä¸­äº§ç”Ÿæœ€å¤§æœ‰æ•ˆçš„è¡Œä¸ºã€‚

### ğŸ‘€ï¸ Motivation

DTå°†RLçœ‹æˆä¸€ä¸ªåºåˆ—å»ºæ¨¡é—®é¢˜ï¼ˆSequence Modeling Problem ï¼‰ï¼Œä¸ç”¨ä¼ ç»ŸRLæ–¹æ³•ï¼Œè€Œä½¿ç”¨ç½‘ç»œç›´æ¥è¾“å‡ºåŠ¨ä½œè¿›è¡Œå†³ç­–ã€‚ä¼ ç»ŸRLæ–¹æ³•å­˜åœ¨ä¸€äº›é—®é¢˜ï¼Œæ¯”å¦‚ä¼°è®¡æœªæ¥Returnè¿‡ç¨‹ä¸­Bootstrappingè¿‡ç¨‹ä¼šå¯¼è‡´Overestimateï¼› é©¬å°”å¯å¤«å‡è®¾;

DTå€ŸåŠ©äº†Transformerçš„å¼ºå¤§è¡¨å¾èƒ½åŠ›å’Œæ—¶åºå»ºæ¨¡èƒ½åŠ›ã€‚

- Decision Transformerçš„è¡¨ç°è¾¾åˆ°ç”šè‡³è¶…è¿‡äº†ç›®å‰æœ€å¥½çš„åŸºäºdynamic programmingçš„ä¸»æµæ–¹æ³•ï¼›
- åœ¨ä¸€äº›éœ€è¦**long-term credit assignment**çš„taskã€ä¾‹å¦‚**sparse reward**æˆ–è€…**delayed reward**ç­‰ã€‘ï¼ŒDecision Transformerçš„è¡¨ç°è¿œè¶…è¿‡äº†æœ€å¥½çš„ä¸»æµæ–¹æ³•.

### ğŸš€ï¸ DTçš„æ ¸å¿ƒæ€æƒ³

![image.png](./assets/image.png)

Decision Transformerçš„æ ¸å¿ƒæ€æƒ³ï¼› Statesã€Actionsã€Returnsè¢«Fed into Modality-Specificçš„çº¿æ€§Embeddingï¼›å¹¶æ·»åŠ äº†å¸¦æœ‰æ—¶é—´æ­¥ä¿¡æ¯çš„positional episodic timestep; è¿™äº›Tokensè¢«è¾“å…¥ä¸€ä¸ªGPTæ¶æ„ï¼Œä½¿ç”¨a causal self-attention maskæ¥é¢„æµ‹actionsã€‚

### ğŸ‰ï¸ DTçš„ä¼˜åŠ¿

1. æ— éœ€Markovå‡è®¾;
2. æ²¡æœ‰ä½¿ç”¨ä¸€ä¸ªå¯å­¦ä¹ çš„Value Functionä½œä¸ºTraining Target;
3. åˆ©ç”¨Transformerçš„ç‰¹æ€§ï¼Œç»•è¿‡é•¿æœŸä¿¡ç”¨åˆ†é…è¿›è¡Œâ€œ**è‡ªä¸¾bootstrapping**â€çš„éœ€è¦ï¼Œé¿å…äº†æ—¶åºå·®åˆ†å­¦ä¹ çš„â€œ**çŸ­è§†**â€è¡Œä¸º;
4. å¯ä»¥é€šè¿‡self-attentionç›´æ¥æ‰§è¡Œä¿¡åº¦åˆ†é…ã€‚è¿™ä¸ç¼“æ…¢ä¼ æ’­å¥–åŠ±å¹¶å®¹æ˜“äº§ç”Ÿå¹²æ‰°ä¿¡å·çš„ Bellman Backup ç›¸åï¼Œå¯ä»¥ä½¿ Transformer åœ¨å¥–åŠ±ç¨€å°‘æˆ–åˆ†æ•£æ³¨æ„åŠ›çš„æƒ…å†µä¸‹ä»ç„¶æœ‰æ•ˆåœ°å·¥ä½œ.

## Dependencies

### 1. [D4RL](https://sites.google.com/view/d4rl/home) ( Dataset for Deep Data-Driven Reinforcement Learning )

### 2. [MUJOCO](https://github.com/deepmind/mujoco/releases/tag/2.1.0) 210

```
# å®‰è£…ä¹‹å‰å…ˆå®‰è£…absl-pyå’Œmatplotlib 
pip install absl-py 
pip install matplotlib 

"""
git clone https://github.com/rail-berkeley/d4rl.git
cd d4rl
pip install -e . # è¿™ç§æ–¹æ³•ä¸å¥½ä½¿ !! 
"""

#é¦–å…ˆåœ¨https://github.com/deepmind/dm_controlè¿™ä¸ªåº“git clone
# cd
pip install -r requirement.txt 
# ç„¶å 
pip install matplotlib 
# ç„¶å https://github.com/takuseno/d3rlpy 
pip install d3rlpy 
# ç„¶åå®‰è£…mujoco 210  
# ç›´æ¥å®‰è£…ï¼Œç„¶åæ·»åŠ ç¯å¢ƒå˜é‡ 
# è£…å®Œä¹‹åè¿›d4rlæ–‡ä»¶å¤¹ä¸‹
python setup.py install 
# æˆåŠŸå®‰è£… d4rl 1.1 
```

### 3. [GPT-2](https://openai.com/blog/better-language-models/)

---

```
pip install transformers
```

## Experiments

### Group1:   Decision Transformer â€” **Hopper-v3-Medium-Dataset**

#### å‚æ•°Config 

```
class Config:
    env = "hopper"
    dataset = "medium"
    mode = "normal" # "delayed" : all rewards moved to end of trajectory
    device = 'cuda'
    log_dir = 'TB_log/'
    record_algo = 'DT_Hopper_v1'
    test_cycles = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')

    # æ¨¡å‹
    model_type = "DT"
    activation_function = 'relu'

    # Scalar
    max_length = 20 # max_len # K
    pct_traj = 1.
    batch_size = 64
    embed_dim = 128
    n_layer = 3
    n_head = 1
    dropout = 0.1
    lr = 1e-4
    wd = 1e-4
    warmup_steps = 1000
    num_eval_episodes = 100
    max_iters = 50
    num_steps_per_iter = 1000

    # Bool
    log_to_tb = True
```

#### æ•ˆæœ

![image.png](./assets/1643383980261-image.png)
